from dataclasses import dataclass
from typing import Callable, Dict, Literal

from langchain_core.messages import AIMessage, SystemMessage
from langchain_core.prompts import HumanMessagePromptTemplate

from agent.output_structure import Copies, ReflectDetails
from agent.state import State
from models.llm import LLM
from utils.node_util import filter_key_from_list, get_output_format_instructions


@dataclass
class NodeType:
    name: str  # ãƒãƒ¼ãƒ‰ã®åå‰
    func: Callable  # ãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹é–¢æ•°


class Node:
    def __init__(
        self,
        llm: LLM,
        prompt: Dict[str, Dict[str, str]],
    ) -> None:

        self.llm = llm
        self.prompt = prompt

        # ================
        # Define Node
        # ================
        self.generate_copy = NodeType("generate_copy", self._generate_copy)
        self.user_select_copy = NodeType("user_select_copy", self._user_input)
        self.reflect_copy = NodeType("reflect_copy", self._reflect_copy)
        self.user_input_additioal_info_copy = NodeType(
            "user_input_additioal_info_copy", self._user_input
        )
        self.end = NodeType("dummy_end", self._end_node)

    # ================
    # Node Functions
    # ================
    def _start_node(self, state: State):
        print("Node: start_node")

    def _generate_copy(self, state: State) -> State:
        print("Node: generate_copy")

        product_info = state["product_info"]

        # åˆå›ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
        if state["iteration_count"] == 0:
            system_prompt = SystemMessage(
                content=self.prompt["generate_copy"]["system"]
            )
            human_prompt = HumanMessagePromptTemplate.from_template(
                self.prompt["generate_copy"]["user_first"]
            ).format(
                product_info=product_info,
                output_format_instruction=get_output_format_instructions(Copies),
            )

            state["messages"] = [system_prompt, human_prompt]
        else:
            human_prompt = HumanMessagePromptTemplate.from_template(
                self.prompt["generate_copy"]["user_second"]
            ).format(
                product_info=product_info,
                additional_info=state["additional_info"],
                additional_info_input=state["additional_info_input"],
                output_format_instruction=get_output_format_instructions(Copies),
                state=state,
            )

        state["messages"].append(human_prompt)

        # invoke
        ai_message = self.llm((state["messages"]), Copies)
        state["messages"].append(AIMessage(ai_message.model_dump_json()))

        output_list = ai_message.model_dump()["copies"]

        # streamlitè¡¨ç¤ºç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        message_text = ""
        for output in output_list:
            # avoid to break markdown format
            output["copy_text"] = output["copy_text"].replace("\n", "")
            # markdownæ”¹è¡Œã®ãŸã‚ç©ºç™½ã‚¹ãƒšãƒ¼ã‚¹ãŒ2ã¤å¿…è¦
            message_text += f"""
        **ã€{output["title"]}ã€‘**\u0020\u0020
        **ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼**ï¼š{output["copy_text"]}\u0020\u0020
        **ç†ç”±**ï¼š{output["reason"]}
        """
        display_message_dict = {
            "title": f"**ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã®ä½œæˆ** {state['iteration_count'] + 1}å›ç›®",
            "icon": "ğŸ“",
            "message_text": message_text,
        }

        # 'reason'ã‚­ãƒ¼ã®ã¿ã‚’å‰Šé™¤ã—ãŸæ–°ã—ã„ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
        filtered_list = filter_key_from_list(output_list, "reason")

        # çŠ¶æ…‹ã®æ›´æ–°
        state["copies"] = filtered_list
        state["display_message_dict"] = display_message_dict

        return state

    def _reflect_copy(self, state: State) -> State:
        print("Node: reflect_copy")

        copies = state["copies"]

        human_prompt = HumanMessagePromptTemplate.from_template(
            self.prompt["reflect_copy"]["user"]
        ).format(
            copies=copies,
            output_format_instruction=get_output_format_instructions(ReflectDetails),
        )

        state["messages"].append(human_prompt)

        # invoke
        ai_message = self.llm((state["messages"]), ReflectDetails)
        state["messages"].append(AIMessage(ai_message.model_dump_json()))

        # æ–‡å­—åˆ—ã‚’Pythonã®è¾æ›¸ã«å¤‰æ›
        data = ai_message.model_dump()

        display_message_dict = {
            "title": f"**ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã®æ”¹å–„** {state['iteration_count'] + 1}å›ç›®",
            "icon": "ğŸ”„",
            "message_text": f"""
            **æ”¹å–„ç‚¹**ï¼š{data["improvement_point"]}\u0020\u0020
            **å¿…è¦ãªè¿½åŠ æƒ…å ±**ï¼š{data["additional_info"]}\u0020\u0020
            **ç†ç”±**ï¼š{data["reason"]}
            """,
        }

        # çŠ¶æ…‹ã®æ›´æ–°
        state["additional_info"] = data["additional_info"]
        state["display_message_dict"] = display_message_dict

        # ã‚«ã‚¦ãƒ³ãƒˆã‚¢ãƒƒãƒ—
        state["iteration_count"] += 1

        return state

    def _user_input(self, state: State):
        pass

    def _end_node(self, state: State):
        print("Node: end_node")
        return {"is_finish": True, "display_message_dict": None}

    # ================
    # Conditional Functions
    # ================
    def should_rethink(self, state: State) -> Literal["reflect", "end"]:
        if state["is_rethink"]:
            return "reflect"
        else:
            return "end"
